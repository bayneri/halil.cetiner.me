---
title: "From Heavy Hitters to Lazy Decay: Learning to Forget at Scale"
layout: post
date: 2025-11-21 00:44
image: /assets/images/notes/heavy-hitters/header.jpg
headerImage: true
tag:
- reliability
- distributed-systems
- algorithms
category: blog
author: bayneri
description: Detecting hot keys in real time is easy to explain and hard to age correctly. This piece is about the moment where counting is not enough and forgetting becomes the real design problem.
---

_Originally published on Medium:_ [From Heavy Hitters to Lazy Decay: Learning to Forget at Scale](https://medium.com/@cetiner/from-heavy-hitters-to-lazy-decay-learning-to-forget-at-scale-20d670650dae)

<p>I’d been reading about how large systems handle <strong>hot keys, </strong>the tiny fraction of IDs or cache entries that suddenly get hammered with traffic.<br>Everyone talks about <em>detecting</em> and <em>caching</em> them automatically, in real time.</p><p>At first, I just wanted to understand how you’d even detect what’s <em>hot right now</em>. I didn’t plan to build anything. I was chasing an idea.</p><p>I started with the obvious tools: a <a href="https://dsf.berkeley.edu/cs286/papers/countmin-latin2004.pdf"><strong>Count-Min Sketch (CMS)</strong></a><strong> </strong>to estimate frequencies, and maybe a small <strong>top-K heap</strong> to keep the heaviest hitters on record.</p><figure><img alt="" src="/assets/images/notes/heavy-hitters/count-min-sketch.gif" /><figcaption><a href="https://redis.io/blog/count-min-sketch-the-art-and-science-of-estimating-stuff/">https://redis.io/blog/count-min-sketch-the-art-and-science-of-estimating-stuff/</a></figcaption></figure><p>It all made sense… until I realised something fundamental: they don’t <em>forget</em>.</p><p>You need a way to know what’s hot now. With this design, once a key becomes hot, it stays hot forever because its old counts never fade. Think of it as YouTube still thinking Gangnam Style is trending or that meme you posted which went viral is still lurking around in Reddit’s cache.</p><p>I needed a <strong>rolling window</strong> view, something like “last 5 minutes.”</p><p>My first thought was to slice time into <strong>buckets</strong>. I thought I can keep 12 CMS instances, one per 5-second slice, and always sum the last twelve to represent the most recent minute.</p><p>That worked in theory, but it felt clumsy. The memory footprint multiplied, merging got messy, and the data jumped discontinuously whenever a bucket rotated out.</p><p>I wanted a smoother sense of time. A system that <em>continuously forgets</em>.</p><p>That’s how I stumbled onto <strong>exponential decay</strong>.</p><p>Exponential decay has a kind of mathematical grace to it. Each count just shrinks by a factor as time passes that guarantees it to be halved in predefined time and fadeout over time.</p><figure><img alt="" src="/assets/images/notes/heavy-hitters/exponential-decay.png" /><figcaption><a href="https://en.wikipedia.org/wiki/Exponential_decay">https://en.wikipedia.org/wiki/Exponential_decay</a></figcaption></figure><p>Of course this wasn’t the answer but I felt I was getting closer. The problem this time is closer to heart as a I spent <em>some time</em> in another life thinking about algorithmic complexities. This algorithm requires <strong>O(N)</strong> work every tick. If N is in the millions, I can build a very elegant CPU heater.</p><p>At that point, I asked ChatGPT about it. It started talking about some kind of wizardry I hadn’t seen before. Maybe it was my rusty CS fundamentals, but it took me a bit to really grasp it. That’s what triggered me to write about it here.</p><blockquote>You don’t have to decay every counter. Just keep a global decay factor.</blockquote><p>At first that sounded impossible. How could a single number replace a million multiplications? Then I worked it out, and it suddenly made perfect sense.</p><p>All exponential decay means is multiplying by the same factor over time, and as I learnt 20+ years ago, multiplication is <strong>commutative</strong>. You can apply it now, later, or never; the math is identical.</p><p>So instead of touching every counter, you maintain one variable:</p><pre>G = 1.0          # global decay factor<br>last_decay = now()<br>λ = 1 / τ        # decay rate</pre><p>Each time time moves forward, update it:</p><pre>Δt = now() - last_decay<br>G *= exp(-λ * Δt)<br>last_decay = now()</pre><p>Then, when adding a new event, scale it by the inverse of that global decay:</p><pre>cms[row][hash_i(key)] += 1.0 / G</pre><p>and when reading, multiply the estimate by G:</p><pre>est = min(cms[row][hash_i(key)] for row in cms_rows)<br>return est * G</pre><p>Mathematically, the scaling cancels → (1 / G) × G = 1</p><p>Every new event counts exactly as <em>one hit now</em>. Everything that happened in the past quietly fades as G shrinks.</p><p>No loops. No background sweeps. The whole structure <em>forgets</em> continuously.</p><p>At that moment, the idea clicked completely. All the counters live in a <em>decayed coordinate system</em>. You’re not storing <em>real</em> counts, you’re storing counts divided by G. As time moves forward, that coordinate system compresses. When you multiply by G again on read, you get the current reality. Elegant, right?</p><p>What fascinates me now is how general the pattern is. It’s one of those ideas that’s both mathematical and deeply practical yet simple. <strong>You can forget continuously without ever looping over your past.</strong></p>
